{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lmUefzNRf0uo"},"outputs":[],"source":["!pip install triton eif==1.0.2"]},{"cell_type":"markdown","source":["Importing data from the GitHub repository"],"metadata":{"id":"FBS7k1KRRqQC"}},{"cell_type":"code","source":["import os\n","# Repository directory name\n","repo_dir = '/content/A-Hybrid-Learning-Approach/'\n","# Check if the directory exists\n","if os.path.isdir(repo_dir) == False:\n","  # Clone the GitHub repository into the current Colab directory\n","  !git clone https://github.com/Maxime1969/A-Hybrid-Learning-Approach.git"],"metadata":{"id":"99HUQpKnBm09"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["CodeT5 vectorization scripts"],"metadata":{"id":"9xRAQbtrmjqb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0RAUJb51PLPf"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch as pt\n","import gc\n","import chardet\n","from torch.nn.utils.rnn import pad_sequence\n","class ClsEmbedding():\n","   \"\"\"\n","    Creates an ClsEmbedding object. This object transforms all the code snippets into a dataframe.\n","\n","    Attributes\n","    ----------\n","    data : dataframe\n","        dataframe is obtained from the csv file containing the file names. the path of the csv file is indicated.\n","    datafile: string\n","       the path to the folder containing the files.\n","    tokenizer: tokenizer\n","       it is obtained from RobertaTokenizer.from_pretrained(model_name)\n","    model: T5ForConditionalGeneration.from_pretrained(model_name)\n","       pre-trained model\n","    batch: int\n","        the number of samples per batch.\n","    Methods\n","    -------\n","    get_embedding()\n","        transforms all the code snippets into a dataframe\n","    get_matrix()\n","        obtain the matrix of embeddings\n","    \"\"\"\n","   def __init__(self, data, datafile, tokenizer, model, batch):\n","\n","     self.data = data\n","     self.datafile = datafile\n","\n","     self.tokenizer = tokenizer\n","     self.model = model\n","     self.stepbatch = 0\n","     self.batch = batch\n","     self.matrix_data= pd.DataFrame(columns=['Matrix'])\n","\n","   #Embedding\n","   def get_embedding(self):\n","     X_data = self.data.iloc[:, 1].tolist()\n","     y_data =self.data.iloc[:, 5].tolist()\n","\n","     if len(X_data)%self.batch == 0:\n","       self.stepbatch = int((len(X_data)/self.batch))\n","     else:\n","       self.stepbatch = int((len(X_data)//self.batch)) + 1\n","\n","     for i in range(int(self.stepbatch)):\n","       batch_samples =[]\n","       generator_embeddings = []\n","       labels_retained=[]\n","       if len(X_data) - i*(self.batch + 1) <= self.batch:\n","          batch_samples = X_data[i*(self.batch + 1): len(X_data)]\n","          batch_label= y_data[i*(self.batch + 1): len(y_data)]\n","\n","       else:\n","        batch_samples = X_data[i*(self.batch + 1):i*(self.batch + 1) + self.batch]\n","        batch_label= y_data[i*(self.batch + 1):i*(self.batch + 1) + self.batch]\n","       for k, batchs in enumerate(batch_samples):  # Loop with step of 'batch'\n","          if os.path.exists(self.datafile + \"/\" + batchs):\n","           with open(self.datafile + \"/\" + batchs, 'rb') as file:\n","            detected_encoding = chardet.detect(file.read())['encoding']\n","           with open(self.datafile + \"/\" + batchs, \"r\", encoding= detected_encoding) as fichier:\n","            contenu = fichier.readlines()\n","            if contenu!=[]:\n","              inputs = self.tokenizer.encode_plus(contenu, padding='longest', truncation=True, return_tensors='pt')\n","              inputs = inputs.to(device)\n","              outputs = self.model(inputs.input_ids, attention_mask=inputs.attention_mask, decoder_input_ids = inputs['input_ids'], output_hidden_states=True )\n","              embedding = outputs.encoder_last_hidden_state\n","              embedding = embedding.to(device)\n","              mean = pt.mean(embedding, dim=(1,2))\n","              std = pt.std(embedding, dim=(1,2))\n","              normalized_embedding = (embedding - mean) / std\n","              normalized_embedding = normalized_embedding.to(device)\n","              reduced_normalized_embedding = pt.mean(normalized_embedding, dim=0).to(device)\n","              fused_normalized_embeddings = pt.mean(reduced_normalized_embedding, dim=0).to(device)\n","              x_normalized = (fused_normalized_embeddings - fused_normalized_embeddings.min()) / (fused_normalized_embeddings.max() - fused_normalized_embeddings.min())\n","              generator_embeddings.append(x_normalized.cpu().detach().numpy())\n","              labels_retained.append(batch_label[k])\n","\n","       df_emb = pd.DataFrame(generator_embeddings)\n","       # Create Create a DataFrame of labels (column named 'label')\n","       df_label = pd.DataFrame(labels_retained, columns=['label'])\n","       # Concatenate horizontally (axis=1) so that the labels are in one column\n","       batch_df = pd.concat([df_emb, df_label], axis=1)\n","       yield batch_df\n","\n","   def get_matrix(self):\n","     data_list = []\n","     generator = self.get_embedding()\n","     for X_data in generator:\n","        df = pd.DataFrame(X_data)\n","        data_list.append(df)\n","     data = pd.concat(data_list, axis=0, ignore_index=True)\n","     return data"]},{"cell_type":"markdown","source":["Data loading methodology"],"metadata":{"id":"BiC9k_MURRuP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tTN90RZASGim"},"outputs":[],"source":["import gc\n","import chardet\n","from transformers import RobertaTokenizer, T5ForConditionalGeneration\n","import requests\n","from sklearn.utils import resample\n","\n","#Setting device\n","if pt.cuda.is_available():\n","  device = \"cuda\"\n","  pt.cuda.empty_cache()\n","  # Enable GPU.\n","  pt.cuda.set_device(0)\n","else:\n","  device =\"cpu\"\n","gc.collect()\n","#Parameter\n","batch = 100\n","#embedding model\n","model_name = 'Salesforce/codet5-small'\n","tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","model = T5ForConditionalGeneration.from_pretrained(model_name)\n","\n","def get_load_test (default_name):\n","\n","  match default_name:\n","    case 'Blob':\n","       data_ = pd.read_csv(repo_dir + '_datablob_.csv', sep =',', index_col = \"Id\")\n","       datafile = repo_dir + 'DataBlob'\n","    case 'LongMethod':\n","       data_ = pd.read_csv(repo_dir + 'data_longmethod.csv', sep =',', index_col = \"Id\")\n","       datafile = repo_dir + 'Data_LongMethod'\n","    case _:\n","       data_ = pd.read_csv(repo_dir + 'data_poltergeist_ant.csv', sep =',', index_col = \"Id\")\n","       datafile = repo_dir + 'File_apache-ant-annoted'\n","\n","  data_ = resample(data_, replace = False, n_samples = len(data_), random_state=42)\n","  embedding = ClsEmbedding(data_, datafile, tokenizer, model, batch)\n","  X_data = embedding.get_matrix()\n","  return X_data\n"]},{"cell_type":"markdown","source":["Loading data"],"metadata":{"id":"RmZZGF11DDuC"}},{"cell_type":"code","source":["X_data_blob = get_load_test('Blob')\n","X_data_longmethod = get_load_test('LongMethod')\n","X_data_poltergeist = get_load_test('Poltergeist')"],"metadata":{"id":"7if-YBfb93vb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implementation of anomaly isolation models"],"metadata":{"id":"fzLVtzTvNsqg"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.neighbors import NearestNeighbors\n","from eif import iForest\n","class iNNE:\n","    def __init__(self, X, t, n):\n","        self.t = t  # Number of samples\n","        self.n = n  # Sample size\n","        self.samples = []\n","        self.X = X\n","        self.rays = []\n","        self.centers = []\n","        self.lesindices = []\n","        self.models = []\n","\n","    def fit(self, X):\n","\n","        # Iterate t times to create t sets of n hyperspheres\n","        for _ in range(self.t):\n","            sample_indices = np.random.choice(len(X), self.n, replace=False)\n","            sample = X[sample_indices]\n","            # Calculate the nearest neighbors\n","            nbrs = NearestNeighbors(n_neighbors=1).fit(sample)\n","            self.models.append((nbrs, sample))\n","\n","    def _isolation_score(self, x, modele, rays):\n","        scores = []\n","        i = 0\n","        # Iterate over each set of samples\n","        for nbrs, sample in modele:\n","           distance, index = nbrs.kneighbors([x])\n","           distance = distance[0][0]\n","           center_index = index[0][0]\n","           center_radius = (rays[i])[center_index]\n","           if distance < center_radius:\n","              # Find the nearest neighbor to the current center\n","              neighbor_index = np.argmin((rays[i]))\n","              neighbor_radius = (rays[i])[neighbor_index]\n","\n","              score = 1 - (neighbor_radius / center_radius)\n","              scores.append(score)\n","           else:\n","              scores.append(1.0)\n","           i+=1\n","        # Return the average of the isolation scores\n","        return np.mean(scores)\n","\n","    def predict(self, X, modele, rays):\n","        scores = [self._isolation_score(x, modele, rays) for x in X]\n","        return np.array(scores)\n","\n","\n","Threshold = [0.6, 0.7, 0.8, 0.9]\n","\n","def get_outliers_iNNE(X_data, X_data_train, X_data_test, Threshold):\n","   listoutliers = []\n","   for threshold in Threshold:\n","      # Create an instance of iNNE\n","      inne = iNNE(t=100, n=256)\n","      inne.rays = []\n","      # Train the model on training data\n","      inne.fit(X_data_train)\n","      for nbrs, sample in inne.models:\n","         distance, _ = nbrs.kneighbors(sample)\n","         raidus = distance.flatten()\n","         inne.rays.append(raidus)\n","      scores = inne.predict(X_data_test, inne.models, inne.rays)\n","      score = (scores - threshold)\n","      #Anomaly prediction\n","      is_inlier = np.ones_like(scores, dtype=int)\n","      is_inlier[scores > threshold] = -1\n","      X_data['anomaly']= is_inlier\n","      outlierseif = [myindex for myindex in X_data.index if X_data.loc[myindex, 'anomaly'] == -1]\n","      listoutliers.append(outlierseif)\n","      X_data = X_data.drop(columns=X_data.columns[-1])\n","   ens_outliers = {item for sublist in listoutliers for item in sublist}\n","   return ens_outliers\n","\n","\n","def get_outliers_EIF(X_data, X_train, ntree, Threshold):\n","  myoutliers = []\n","  for epoch in range(100):\n","     listoutliers = []\n","     for threshold in Threshold:\n","        clf_extended = iForest(X_train, ntree, sample_size=256)\n","        # Anomaly Score for EIF\n","        scores_extended = clf_extended.compute_paths(X_in= None)\n","        scores = -(scores_extended - threshold)\n","        #Anomaly prediction for EIF\n","        is_inlier = np.ones_like(scores_extended, dtype=int)\n","        is_inlier[scores <  0] = -1\n","        X_data['anomaly']= is_inlier\n","        outlierseif = [myindex for myindex in X_data.index if X_data.loc[myindex, 'anomaly'] == -1]\n","        listoutliers.append(outlierseif)\n","        X_data = X_data.drop(columns=X_data.columns[-1])\n","     ens_outliers = {item for sublist in listoutliers for item in sublist}\n","     myoutliers.append((len(ens_outliers), list(ens_outliers)))\n","  number_outliers, list_outliers = zip(*myoutliers)\n","  number_outliers = np.array(number_outliers)\n","  return number_outliers, list_outliers\n",""],"metadata":{"id":"YkHTykJfD98T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_3O1Ill5FYdp"},"source":["Definition of ensemble learning models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jobh6Fm6huod"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.ensemble import  BaggingClassifier, StackingClassifier, GradientBoostingClassifier, RandomForestClassifier\n","from imblearn.ensemble import RUSBoostClassifier\n","\n","# Define stratified k-fold cross-validation\n","cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","\n","# Define the basic model for bagging\n","_estimator =  RandomForestClassifier(n_estimators = 100, random_state = 42)\n","#Apply the bagging\n","bagging_clf = BaggingClassifier(estimator=_estimator, n_estimators=10, max_samples=1.0, max_features=1.0, bootstrap=True, random_state=42, verbose = False)\n","# Define the basic model for stacking\n","models = []\n","n_models = 10\n","for i in range(n_models):\n","    estimator = RandomForestClassifier(n_estimators=100 + i*50, max_depth=5 + i*2, min_samples_split=2 + i, random_state=42 + i)\n","    models.append((f'estimate_{i}',  estimator))\n","# Define the final estimators\n","final_estimator_lr = LogisticRegression(solver='liblinear', random_state=42)\n","# Initialize the StackingClassifier\n","stacking_lr = StackingClassifier(estimators=models, final_estimator=final_estimator_lr, cv=cv, stack_method='predict_proba', n_jobs=-1, verbose=False)\n","# Define the basic model for boosting\n","base_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n","# Apply RUSBoost\n","rus_boost = RUSBoostClassifier(estimator=base_model, n_estimators=10, random_state=42)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}